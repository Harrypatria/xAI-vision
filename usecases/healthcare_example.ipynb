# XAI-Vision: Healthcare Use Case Example

This notebook demonstrates how XAI-Vision can be applied in a healthcare context to create explainable models for disease prediction. We'll train a diabetes prediction model and use XAI-Vision to explain its behavior.

## 1. Setup and Data Loading

First, let's import the necessary libraries and load the diabetes dataset.


```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
from sklearn.preprocessing import StandardScaler

# Import XAI-Vision
import xai_vision as xv

# Load the diabetes dataset
from sklearn.datasets import load_diabetes

# Set random seed for reproducibility
np.random.seed(42)
```

Now, let's load and prepare the diabetes dataset:


```python
# Load the diabetes dataset
diabetes = load_diabetes()
X = diabetes.data
y = diabetes.target

# Convert this to a classification problem (threshold at median)
y_binary = (y > np.median(y)).astype(int)

# Get feature names
feature_names = diabetes.feature_names
class_names = ['Normal', 'Diabetic']

# Create a DataFrame for easier data exploration
df = pd.DataFrame(X, columns=feature_names)
df['target'] = y_binary

# Display basic information
print(f"Dataset shape: {df.shape}")
print("\nFeature descriptions:")
for i, name in enumerate(feature_names):
    print(f"- {name}: {diabetes.DESCR.split(name)[1].split('\n')[0]}")
    
print("\nClass distribution:")
print(df['target'].value_counts())

# Preview the data
df.head()
```

## 2. Data Preprocessing and Model Training

Let's preprocess the data and train a Random Forest Classifier:


```python
# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(
    X, y_binary, test_size=0.2, random_state=42
)

# Scale the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Train a Random Forest Classifier
model = RandomForestClassifier(
    n_estimators=100, 
    max_depth=5,
    random_state=42
)
model.fit(X_train_scaled, y_train)

# Evaluate the model
y_pred = model.predict(X_test_scaled)
y_prob = model.predict_proba(X_test_scaled)[:, 1]

print("Classification Report:")
print(classification_report(y_test, y_pred, target_names=class_names))

print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred))

print(f"\nROC AUC Score: {roc_auc_score(y_test, y_prob):.4f}")
```

Great! We've trained a Random Forest model to predict diabetes. Now, let's use XAI-Vision to explain the model's behavior.

## 3. Creating an Explainer for the Model

Now we'll use XAI-Vision to create an explainer for our diabetes prediction model:


```python
# Create an explainer
explainer = xv.ModelExplainer(
    model=model,
    X=X_train_scaled,
    y=y_train,
    feature_names=feature_names,
    class_names=class_names,
    model_name="Diabetes Prediction Model",
    task_type="classification"
)

# Fit the explainer (compute SHAP values, etc.)
explainer.fit(compute_shap_values=True)
```

## 4. Explaining the Model Globally

Let's first look at global explanations to understand the overall behavior of the model:


```python
# Plot feature importance
fig_importance = explainer.plot_feature_importance(top_n=10)
fig_importance.show()
```

We can see the most important features for predicting diabetes. Now let's look at a SHAP summary plot to understand how each feature contributes to the predictions:


```python
# Plot SHAP summary
fig_shap_summary = explainer.plot_shap_summary()
fig_shap_summary.show()
```

The SHAP summary plot shows how each feature impacts the model output. Red points represent high feature values, blue points represent low values, and the horizontal position shows the impact on the model prediction.

## 5. Understanding Feature Interactions

Let's explore how specific features affect the model's predictions:


```python
# Plot partial dependence for the most important features
top_features = explainer.explainers['feature_importance'].get_top_features(3)

for feature in top_features:
    fig_pdp = explainer.plot_partial_dependence(feature)
    fig_pdp.show()
```

We can also investigate individual conditional expectation (ICE) curves to understand how predictions change for individual patients:


```python
# Plot ICE curves for the most important feature
most_important_feature = top_features[0]
fig_ice = explainer.plot_ice_curves(most_important_feature, num_instances=30)
fig_ice.show()
```

## 6. Explaining Individual Predictions

Now, let's explain predictions for specific patients:


```python
# Select a patient from the test set
patient_idx = 5
patient_features = X_test_scaled[patient_idx]
patient_label = y_test[patient_idx]
patient_prediction = model.predict_proba([patient_features])[0, 1]

print(f"Patient #{patient_idx}")
print(f"Actual class: {class_names[patient_label]}")
print(f"Predicted probability of diabetes: {patient_prediction:.4f}")
print("\nFeature values:")
for name, value in zip(feature_names, X_test[patient_idx]):
    print(f"- {name}: {value:.4f}")
```

Let's now explain the prediction for this patient:


```python
# Get LIME explanation for the patient
fig_lime = explainer.plot_lime_explanation(patient_features)
fig_lime.show()
```

The LIME explanation shows which features contributed most to this specific prediction. Let's also look at the SHAP explanation:


```python
# Get comprehensive explanation for the patient
explanation = explainer.explain_instance(
    instance=patient_features,
    explanation_types=['shap', 'lime']
)

# We can access the raw explanation data if needed
# shap_values = explanation['shap']['shap_values']
```

## 7. Creating an Interactive Dashboard

Finally, let's create an interactive dashboard to explore the model explanations:


```python
# Create an interactive dashboard
dashboard = explainer.explain_dashboard(
    title="Diabetes Prediction Model Explanation",
    description="This dashboard explains how the Random Forest model predicts diabetes risk."
)

# Serve the dashboard (this will open in a new browser tab)
dashboard.serve()
```

## 8. Clinical Relevance and Applications

This explainable diabetes prediction model has several potential applications in clinical settings:

1. **Risk Factor Identification**: Clinicians can identify which factors contribute most to an individual patient's risk of diabetes.

2. **Personalized Interventions**: By understanding which factors are most important for a specific patient, clinicians can design targeted interventions.

3. **Patient Education**: The visual explanations can be used to educate patients about their risk factors and the importance of lifestyle changes.

4. **Model Validation**: Clinicians can validate the model by confirming whether the identified risk factors align with clinical knowledge.

5. **Regulatory Compliance**: The explainability features help meet regulatory requirements for transparent AI in healthcare.

## 9. Conclusion

In this notebook, we've demonstrated how XAI-Vision can be used to create an explainable diabetes prediction model. The toolkit provides a comprehensive set of explainability methods that help understand both global model behavior and individual predictions.

The combination of feature importance, SHAP, PDP, ICE, and LIME explanations provides a holistic view of the model's decision-making process, which is crucial for healthcare applications where transparency and trustworthiness are paramount.
